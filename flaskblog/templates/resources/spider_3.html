{% extends 'Base.html'%}

{% block head %}
<style>
  body {background: #F5F5F5 center top;
}
</style> 
{% endblock %}

{% block body %}
<div class="row">
<div class="container bg-light">

<h1>爬虫系列课程（三）解析请求的文本内容</h1>
<h2>目录</h2>
<ul>
<li><a href="#1">正则表达式解析文本内容</a></li>
<li><a href="#2">BeautifulSoup解析文本内容</a></li>
</ul>
<p>本节内容将助你掌握：</p>
<ul>
<li>正则表达式的基本用法。</li>
<li>Python BeautifulSoup 解析 HTML 标签的方法。</li>
</ul>
<p>话不多说，上车！</p><br>
<h2 id="1">正则表达式解析文本内容</h2>
<p>在第一节和第二节中我们对爬虫的第一步：像网页发送请求获取内容有了一定的认识，今天我会像大家介绍爬虫的第二步：对获取的文本内容进行解析，而这就需要用到我们今天的主角之一：正则表达式。让我们先看看正则表达式的定义是什么，我们引用一下菜鸟教程的描述：</p>
<blockquote>
<p>正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等。</p>
</blockquote>
<p>简单来说，正则表达式的价值在于我们可以再海量的文本中获取带有一定特征的字符串。由于这个强大的功能正则表达式的使用范围不仅仅是爬虫，适用的语言也绝不仅是Python，我先举个例子大家看看正则表达式是怎样运作的。</p>
<p>比如说我要匹配一个密码，我们需要他只有大小写字母和数字组成，长度在6-20之间我们可以这样来：</p>
<pre><code class="python">
&gt;&gt;&gt; import re
&gt;&gt;&gt; username = 'ILovePython'
&gt;&gt;&gt; m1 = re.match(r'^[0-9a-zA-Z_]{6,20}$', username)
&gt;&gt;&gt; print(m1)
&lt;re.Match object; span=(0, 11), match='ILovePython'&gt;
&gt;&gt;&gt; username_2 = '**hhhh**'
&gt;&gt;&gt; m2 = re.match(r'^[0-9a-zA-Z_]{6,20}$', username_2)
&gt;&gt;&gt; print(m2)
None

</code></pre>
<p>re就是python使用正则表达式的库，它是 python 自带的原生库，我们不需要用 pip 安装，直接 import 就可以了。</p>
<p>看下我们写的正则表达式。r代表禁用转义字符，由于我们的正则表达式可能会用到''，我们一般习惯在前面加上r。而^,$则是代表我们的匹配范围是从文本的开始到结束。中括号内的内容是我们的匹配范围，在这里是所有的数字和大小写字母。而大括号里面就是我们的长度范围，这里是6到20。</p>
<p>这里有一个需要注意的地方，大家可能有逗号之后空一格的习惯，但这在我们正则表达式中是会有影响的：</p>
<pre><code class="python">
&gt;&gt;&gt; username = 'abcdef123'
&gt;&gt;&gt; m1 = re.match(r'^[0-9a-zA-Z_]{6, 20}$', username)
&gt;&gt;&gt; print(m1)
None

</code></pre>
<p>大家可以看到如果我们的20前面加了空格会影响匹配结果。</p>
<p>现在我们来看看返回值：当我们匹配成功，会返回一个re.Match的对象，而如果匹配失败，我们将返回一个None值。</p>
<p>这是正则表达式的最基本用法，当然我们在实际应用中不可能一个个地去匹配单个字符串，我们会在海量文本中获取我们需要的字符串，这就要用到 re.compile 和 re.findall 方法了，让我们看一个匹配中国大陆手机号码的例子</p>
<pre><code class="python">
&gt;&gt;&gt; pattern = re.compile(r'(?&lt;=\D)1[34578]\d{9}(?=\D)')
&gt;&gt;&gt; sentence = '''
... 今天我们来探讨一下第268532213页的第895543331
... 个问题。我的手机号是13813895138，我之前的手机号
... 已经弃用了，是13913950139，如果大家遇到危险要
... 拨打110或119，或者拨打19119119191'''
&gt;&gt;&gt; mylist = re.findall(pattern,sentence)
&gt;&gt;&gt; print(mylist)
['13813895138', '13913950139']

</code></pre>
<p>用re.compile方法设置了一个匹配模式，\D代表非数字字符，(?&lt;=\D)的意思是以非数字字符开始，同样的(?=\D)代表以非数字字符结束，中间的一串代表<strong>以1开始，第二个数字是3，4，5，7，8的其中一个，再加上九个任意数字</strong>。我们用 re.findall 将这个pattern 匹配我们的sentence，成功找到了其中的两个合法手机号。</p>
<p>进行一个小实战，我们来看看具体在爬虫工作中是怎么做的。</p>
<p>先来个简单的，获取必应搜索'python爬虫'搜索页面的所有标题，我们先看看我们网页的内容，右键点击标题，然后点一下检查，会看到：</p>
<br><img src="{{ url_for('static', filename='images/spider/3.4.JPG') }}" style="width: 750px;height: 500px;" class="spider" ><br><br>
<p>Elements 页面下的内容就是爬虫获取的源代码的文本内容。右键点击每一个搜索内容，选择检查，可以看到每一个搜索内容的标题藏在标签a中，而装载着网页标题的a标签，有特定的target值 '_blank'，我们在爬虫会遇到各种各样的这样标签后的值 id, class, target, property。如果大家对 HTML 不太熟悉可能不太理解这些值有什么意义，不过没关系，大家可以暂时理解为这是区分标签名相同的身份标识。</p>
<p>依旧用 re.compile 写出正则表达式, 用requests发送请求：</p>
<pre><code class="python">
import re
import requests

url = 'https://cn.bing.com/search?q=python爬虫'
headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0&quot;}
pattern = re.compile(r'&lt;h2&gt;&lt;a target=&quot;_blank&quot;.*?&gt;(.*?)&lt;/a&gt;&lt;/h2&gt;')
r = requests.get(url, headers=headers)
if r.status_code == 200:
    headline_list = re.findall(pattern, r.text)
    for headline in headline_list:
        print(headline)
</code></pre>
<p>看下输出结果：</p>
<br><img src="{{ url_for('static', filename='images/spider/3.5.JPG') }}" style="width: 450px;height: 135px;" class="spider" ><br><br>
<p>我们的搜索内容：Python爬虫被 strong 标签包围着，这个标签代表其中的内容很重要，表现为加粗。我们不想要这个东西，这个时候我们可以用 re.sub 方法替换掉。我们在输出前加上：</p>
<pre><code class="python">
headline = re.sub(r'&lt;.*?&gt;','',headline)

</code></pre>
<p>我们把&lt;&gt;及其之间的内容替换为空字符串，再看看输出：</p>
<br><img src="{{ url_for('static', filename='images/spider/3.7.JPG') }}" style="width: 340px;height: 155px;" class="spider" ><br><br>
<p>成功了！</p>
<p>你以为正则表达式只有这点内容？才怪！来，看看正则表达式的各种操作：</p>
<br><img src="{{ url_for('static', filename='images/spider/3.1.PNG') }}" style="width: 700px;height: 700px;" class="spider" ><br><br>
<br><img src="{{ url_for('static', filename='images/spider/3.2.PNG') }}" style="width: 700px;height: 600px;" class="spider" ><br><br>
<br><img src="{{ url_for('static', filename='images/spider/3.3.PNG') }}" style="width: 750px;height: 450px;" class="spider" ><br><br>
<p>这才是正则表达式的全部操作，大家也不需要去全部记下来，用到的时候再查即可，用多了自然就记下来了。</p><br>
<h2 id="2">BeautifulSoup解析文本内容</h2>
<p>讲完了正则表达式，让我们来看下 BeautifulSoup 是怎么用的。在初步接触正则表达式的时候大家可能对如何写出正确的正则表达式而纠结，而为了解决这个问题，我们可以</p>
<p>放弃他！</p>
<p>在一定程度上可以代替正则表达式的 BeautifulSoup 是一个文本解析器：</p>
<br><img src="{{ url_for('static', filename='images/spider/3.6.JPG') }}" style="width: 700px;height: 300px;" class="spider" ><br><br>
<p>我们用python的标准库即可，pip install bs4 走起</p>
<p>直接上手，看看代码：</p>
<pre><code class="python">
from bs4 import BeautifulSoup
import requests

url = 'https://cn.bing.com/search?q=python爬虫'
headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0&quot;}
r = requests.get(url, headers=headers)
if r.status_code == 200:
    soup = BeautifulSoup(r.text, 'html.parser')
    print(soup.find('a'))
#&lt;a class=&quot;b_logoArea&quot; h=&quot;ID=SERP,5026.1&quot; href=&quot;/?FORM=Z9FD1&quot;&gt;&lt;h1 aria-label=&quot;打开必应主页&quot; class=&quot;b_logo&quot; title=&quot;返回到必应搜索&quot;&gt;&lt;/h1&gt;&lt;/a&gt;

</code></pre>
<p>'html.parser'参数代表的是我们将r.text进行HTML解析，对我们的解析结果进行 find('a') 操作就是找到第一个a标签，如果我们想找到所有a标签呢？</p>
<pre><code class="python">
a_list = soup.find_all('a')
for a in a_list:
    print(a)
# &lt;a class=&quot;b_logoArea&quot; h=&quot;ID=SERP,5026.1&quot; href=&quot;/?FORM=Z9FD1&quot;&gt;&lt;h1 aria-label=&quot;打开必应主页&quot; class=&quot;b_logo&quot; title=&quot;返回到必应搜索&quot;&gt;&lt;/h1&gt;&lt;/a&gt;
# &lt;a aria-label=&quot;清除本文&quot; href=&quot;javascript:void(0);&quot; id=&quot;sw_clx&quot; role=&quot;button&quot; tabindex=&quot;0&quot;&gt;&lt;div class=&quot;sw_close&quot;&gt;&lt;/div&gt;&lt;/a&gt;
...

</code></pre>
<p>我们找到了所有的 a 标签。现在我们进一步来看，如果我们需要特定的a标签，比如我们只要 target 值等于&quot;_blank&quot;的a标签：</p>
<pre><code class="python">
a_list = soup.find_all('a', target=&quot;_blank&quot;)
for a in a_list:
    print(a)
# &lt;a h=&quot;ID=SERP,5090.1&quot; href=&quot;http://c.biancheng.net/view/2011.html&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Python爬虫&lt;/strong&gt;入门教程：超级简单的&lt;strong&gt;Python爬虫&lt;/strong&gt;教程&lt;/a&gt;
#&lt;a h=&quot;ID=SERP,5103.1&quot; href=&quot;https://zhuanlan.zhihu.com/p/73742321&quot; target=&quot;_blank&quot;&gt;干货！&lt;strong&gt;python爬虫&lt;/strong&gt;100个入门项目 - 知乎&lt;/a&gt;
#&lt;a h=&quot;ID=SERP,5115.1&quot; href=&quot;https://www.runoob.com/w3cnote/python-spider-intro.html&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Python 爬虫&lt;/strong&gt;介绍 | 菜鸟教程 - RUNOOB.COM&lt;/a&gt;
#&lt;a h=&quot;ID=SERP,5128.1&quot; href=&quot;https://zhuanlan.zhihu.com/p/21479334&quot; target=&quot;_blank&quot;&gt;如何学习&lt;strong&gt;Python爬虫&lt;/strong&gt;[入门篇]？ - 知乎&lt;/a&gt;
...

</code></pre>
<p>我们找到了所有包含搜索内容链接的标签，现在我们如果想获得所有这些标签里的网址,我们观察一下我们的a标签，里面有一个 href 参数，每个标签的 href 参数都是一个网址，这个应该就是我们搜索内容的链接，我们可以用get获得它：</p>
<pre><code class="python">
a_list = soup.find_all('a', target=&quot;_blank&quot;)
for a in a_list:
    print(a.get('href'))
#http://c.biancheng.net/view/2011.html
#https://zhuanlan.zhihu.com/p/73742321
#https://www.runoob.com/w3cnote/python-spider-intro.html
...

</code></pre>
<p>其他的参数大家依照这个例子即可，不同的参数要用逗号隔开。有一点需要注意的说是<strong>当我们用class参数时，要这样写class_=&quot;...&quot;，这是为了不与python的类起冲突。</strong></p>
<p>如果大家对 css 语言的选择器足够熟悉的话，我们还可以进行这样的操作：</p>
<pre><code class="python">
print(soup.select(&quot;title&quot;))
print(soup.select(&quot;body a&quot;))
print(soup.select(&quot;p &gt; #link1&quot;))

</code></pre>
<p>如果我们想获得a标签里所有的文字内容，我们可以：</p>
<pre><code class="python">
a_list = soup.find_all('a', target=&quot;_blank&quot;)
for a in a_list:
    print(a.get_text())
# Python爬虫入门教程：超级简单的Python爬虫教程
# 干货！python爬虫100个入门项目 - 知乎
# Python 爬虫介绍 | 菜鸟教程 - RUNOOB.COM
# Python 3 爬虫学习笔记 （一） - 简书
# 如何学习Python爬虫[入门篇]？ - 知乎
# Python网络爬虫（一）- 入门基础 - 简书
# Python爬虫学习之（一）| 从零开始 - Python数据科学 ...
# Python爬虫原理 - Python开发之路 - 博客园
# 你是如何开始能写 Python 爬虫？ - 知乎
# 网页爬虫教程系列 | 莫烦Python

</code></pre>
<p>看，我们达到了和正则表达式一样的效果，而我们需要的可能仅仅是理解：find，all，get，text的英文含义……</p>
<p>在实际爬虫中，用BeautifulSoup解析可能用的更多，但是正则表达式在网页源代码的标签设置不明的情况下依然可以替代BeautifulSoup起到显著的作用，两个具体的应用场景大家还需要在实践中多多体会。</p>
<p>当然，你要是对正则表达式和 BeautifulSoup 都不满意，我们后面还会接触更强大的 lxml，敬请期待。</p>
<hr>
<p>推荐资料：</p>
<ul>
<li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup官方文档</a></li>
<li><a href="https://www.runoob.com/regexp/regexp-syntax.html">正则表达式菜鸟教程</a></li>
</ul>
<hr>
<p>我们的爬虫系列课程第三节结束了，在下一节中我将带大家进行第一次爬虫实战，爬取豆瓣电影排行榜上的250部电影。如果你喜欢本系列课程，可以关注下方微信公众号，或者来我的博客：<a href="www.ivblog.tech">www.ivblog.tech </a>我会分享一些其他方面的文章。</p>
</div>
</div>
</div>
{% endblock %}
