{% extends 'Base.html'%}

{% block head %}
<style>
  body {background: #F5F5F5 center top;
}
</style> 
{% endblock %}

{% block body %}
<div class="row">
<div class="container bg-light">
    <h1>爬虫系列课程（一）发送请求</h1><br>
    <h2>目录</h2>
    <ul>
    <li><a href="#1">什么是爬虫</a></li>
    <li><a href="#2">发送请求</a></li>
    <li><a href="#3">什么是GET请求</a></li>
    <li><a href="#4">什么是POST请求</a></li>
    </ul>
    <br>
    <p><em>P.S. 本系列针对的是没有爬虫基础或者在爬虫方面了解相对较少的同学，如有疏漏欢迎大家指正。</em></p>
    <hr>
    <p>本节内容将助你掌握：</p>
    <ul>
    <li>爬虫的基本概念和步骤。</li>
    <li>发送请求的操作和请求的基本内容。</li>
    <li>GET 请求和 POST请求初步了解。</li>
    </ul>
    <br>
    <p>在这个铺天盖地都是python广告的时代，爬虫这个概念也随之而流传的越来越广。那么什么是爬虫，究竟该怎么爬虫，今天我就带大家来初窥爬虫的面目,让我们一起走向爬虫：从入门到<s>精通</s>&lt;&gt;监狱&lt;/font&gt;的旅程。</p>
    <br>
    <h2 id="1">什么是爬虫</h2>
    <p>首先先明确当我们提到爬虫的时候，我们究竟提到的具体步骤包括哪些，宏观来讲，每一个爬虫都分为以下几个步骤：</p>
    <ol>
    <li>像目标网页发送请求获取内容</li>
    <li>对获取的内容进行适当的选取和解析</li>
    <li>将经过处理的内容保存到本地</li>
    <li>将保存下来的内容进行二次加工和分析</li>
    </ol>
    <p>或者用跟概括化的语言说：<strong>爬虫是一个自动在网络上收集信息的程序</strong>，是一整套关于请求，处理，存储的程序，涉及到大量的计算机网络，数据结构方面的知识。</p>
    <p>那么爬虫究竟是用来做什么的呢，最通用的有下面几个应用场景：</p>
    <ol>
    <li>取悦自己：想看小说了，爬下来，想看动漫了；爬下来；不知道看啥？去找一些榜单把他们的数据爬下来；想念 FBI Warning 了……。</li>
    <li>服务大众：爬取下来的信息你可以做网站，对数据进行例如可视化分析，数据特点分析，情感极性分析可以得到很多很有价值的结论。</li>
    <li>训练机器：机器学习需要的大量样本的参与，大数据技术更是需要大量的数据，直接由自己获取显然效率是不高的，但是如果利用爬虫程序适当获取就能事半功倍。</li>
    </ol>
    <p>当然大家在爬虫时要注意，爬虫是有限制和边界的，我们的爬虫无疑给系统的维护增大了成本，我们收集的一些信息如果作于商用，无疑也是对爬取方的一种隐形伤害。也正因为如此，很多网站给我们设置了很多反爬虫手段，这些我们会在以后一一介绍到。</p>
    <p>现在大家对爬虫有了一个模糊的印象，那么我们就从爬虫的第一步，像目标网页发送请求开始吧</p><br>
    <h2 id="2">发送请求</h2>
    <p>首先想让我们用Chrome打开一个网页人人都能上的网页：</p>
    <br><img src="{{ url_for('static', filename='images/spider/1.1.JPG') }}" style="width: 700px;height: 400px;"  class="spider"><br><br>
    <p>（你以为我要用百度吗，too young too simple)</p>
    <p>按 F12 打开开发者模式，看到这里花花绿绿好多东西，我们先选中 Network，然后在搜索栏输入python爬虫：</p>
    <br><img src="{{ url_for('static', filename='images/spider/1.2.JPG') }}" style="width: 1000px;height: 500px;" class="spider"><br><br>
    <p>Network一栏出现了很多行数据，每一行数据都代表我们像必应服务器发送的一个请求。（当然我们在模拟发送请求时不需要每个请求都发送）服务器收到这些请求返回给我们数据，由网站的HTML文件以特定的样式将数据呈现给我们，就构成了我们看到的搜索页面。</p>
    <p>对于每一行数据，第一列 name 就是这个请求的URL，这个我们在后面详细来看看它是由什么构成的。</p>
    <p>第二列 status code，是我们的响应码，说几个常见的你们就知道响应码是啥了：404，403，500... 响应码就是我们的请求的响应情况，这满满的200自然代表的就是成功请求了。</p>
    <p>后面几列的 type，size，time 想必大家都猜得出来是什么意思。至于 Initiator 标记请求是由哪个对象或进程发起的（请求源），这个大家大概有个印象就行。</p>
    <p>我们具体挑一个请求来看:</p>
    <br><img src="{{ url_for('static', filename='images/spider/1.3.JPG') }}" style="width: 1000px;height: 500px;" class="spider"><br><br>
    <h2 id="3">什么是GET请求</h2>
    <p>HTTP协议的请求方式有: GET， POST，这两个是最常见的请求，还有诸如: PUT，DELETE， HEAD， OPTION我们今天暂且不谈。</p>
    <p>从右侧的 General 一栏中我们可以看到我们选中的请求就是一个 GET请求 他的URL是：</p>
    <p>https://cn.bing.com/search?q=python%E7%88%AC%E8%99%AB&amp;qs=n&amp;form=QBRE&amp;sp=-1&amp;pq=pythonpa%27chon&amp;sc=0-13&amp;sk=&amp;cvid=1E4377E6AA8F486AB7678970E1B58DBB</p>
    <p>http 是我们的互联网超文本传输协议，大家不理解也没关系，可以默认它是我们在的一种数据传输的规则。</p>
    <p>cn.bing.com 是必应服务器的域名，大家可以把它理解为服务器的门牌号码，有了门牌号码我们的电脑才知道请求应该发给谁。</p>
    <p>后面的一大串就是我们像这个服务器访问的网址了，每一个问号后面都是一个携带的参数。聪明的你肯定发现了我们的第一个参数恰好是我们输入的搜索内容，虽然这后面还有很多参数，但我们隐约觉得应该只有第一个参数是决定性的。</p>
    <p>验证一下，把第一个参数的内容换成 人生苦短我用Python：</p>
    <br><img src="{{ url_for('static', filename='images/spider/1.4.JPG') }}" style="width: 900px;height: 500px;" class="spider"><br><br>
    <p>成功了！</p>
    <p>回想一下我们究竟做了什么，我们向必应服务器发送了一个 GET 请求，同时我们携带了一个参数 q= ... ，服务器收到我们的请求，根据我们的参数向我们返回资源，这就是 GET 请求大概的运行模式。</p>
    <p>在我们请求页面的右边，可以看到有很多诸如 request header， response header之类的信息，我们在日后会逐步接触到他们的用途，今天我们先跳过。</p>
    <p>接下来，让我们先看看 POST请求是怎样的</p><br>
    <h2 id="4">什么是POST请求</h2>
    <p>让我们打开必应的登录界面，同样的，进入开发者模式的Network一栏，利用默认的微软账号登陆，大家注意在我们登陆成功的瞬间，出现了这样一闪而逝的画面：</p>
    <br><img src="{{ url_for('static', filename='images/spider/1.6.JPG') }}" style="width: 750px;height: 500px;" class="spider" ><br><br>
    <p>这个神秘的一闪而逝的请求就是我们的 POST 请求，由于我的手速实在无法支撑我在一闪而逝的瞬间展开他的详细信息，于是我用了代理软件呈现给你们：</p>
    <br><img src="{{ url_for('static', filename='images/spider/1.5.JPG') }}" style="width: 700px;height: 380px;"  class="spider"><br><br>
    <p>我们可以看到这个请求的类型是 POST ，他携带了我们刚才输入的username和password。</p>
    <p>我们再想想发生了什么，当我们申请一些资源的时候服务器设了一道门槛，叫登录。我们为了登录，向服务器发送了一个 POST 请求，携带了我们的身份证（用户名）和准入证（密码）（没有准入证身份证给别人捡去咋办），如果我们传入的是正确的参数，服务器收到以后就会把门槛撤走，我们登录成功。</p>
    <p>看到这里肯定有人想问：那 GET 请求和 POST 请求的区别是什么呢，他们不都是携带参数向服务器请求资源吗，我们引用一下 w3schools 的标准答案：</p>
    <ul>
    <li>GET在浏览器回退时是无害的，而POST会再次提交请求。</li>
    <li>GET产生的URL地址可以被Bookmark，而POST不可以。</li>
    <li>GET请求会被浏览器主动cache，而POST不会，除非手动设置。</li>
    <li>GET请求只能进行url编码，而POST支持多种编码方式。</li>
    <li>GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。</li>
    <li>GET请求在URL中传送的参数是有长度限制的，而POST么有。</li>
    <li>对参数的数据类型，GET只接受ASCII字符，而POST没有限制。</li>
    <li>GET比POST更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息。（当然一旦像我们这样抓包他们都得凉凉）</li>
    <li>GET参数通过URL传递，POST放在Request body中。</li>
    </ul>
    <p>看不懂？没关系。先记住一点：<strong>Get是从服务器上获取数据，Post是向服务器传送数据</strong>。其他细节大家在自己写爬虫或者自学计算机网络时自然就慢慢体会到了。</p>
    <hr><br>
    <p>我们的爬虫系列课程第一节结束了，在下一节中我将带大家用Python的requests库来进一步学习发送请求，如果你喜欢本系列课程，可以关注下方微信公众号，或者来我的博客：www.ivblog.tech 我会分享其他方面的一些文章。</p>    
</div>
</div>
</div>
{% endblock %}
